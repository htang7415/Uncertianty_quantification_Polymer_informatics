{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../functions')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from data_processing import load_and_process_data\n",
    "from models import NeuralNetwork, train_ensemble, predict_ensemble\n",
    "from metrics import calculate_metrics, calculate_observed_confidence\n",
    "from post_processing import create_errorbar_plot, plot_abs_error_vs_std, plot_std_histogram, plot_calibration_curve\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 250\n",
    "N_ENSEMBLES = 20\n",
    "NEURONS = (256, 512, 256)\n",
    "LEARNING_RATE = 0.0003\n",
    "num_zero_threshold = 3600\n",
    "\n",
    "# Load and process data\n",
    "file_path = \"../data/other_property/Tm.csv\"\n",
    "X_count, Y = load_and_process_data(file_path, num_zero_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "xtrain, xtemp, ytrain, ytemp = train_test_split(X_count, Y, test_size=0.2, random_state=11)\n",
    "xval, xtest, yval, ytest = train_test_split(xtemp, ytemp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_dataloader(x, y, batch_size):\n",
    "    tensor_x = torch.FloatTensor(x.values).to(device)\n",
    "    tensor_y = torch.FloatTensor(y).unsqueeze(1).to(device)\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = create_dataloader(xtrain, ytrain, BATCH_SIZE)\n",
    "val_loader = create_dataloader(xval, yval, BATCH_SIZE)\n",
    "test_loader = create_dataloader(xtest, ytest, BATCH_SIZE)\n",
    "\n",
    "# For training, create a separate shuffled loader\n",
    "train_loader_shuffled = DataLoader(TensorDataset(torch.tensor(xtrain.values).float(), torch.tensor(ytrain).float()), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Train ensemble\n",
    "start_time = time.time()\n",
    "\n",
    "models = [NeuralNetwork(xtrain.shape[1], *NEURONS).to(device) for _ in range(N_ENSEMBLES)]\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Training Model {i+1}/{N_ENSEMBLES}\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    train_ensemble(model, optimizer, train_loader, train_loader_shuffled, EPOCHS)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "mean_train, std_train, actual_train = predict_ensemble(models, train_loader)\n",
    "mean_test, std_test, actual_test = predict_ensemble(models, test_loader)\n",
    "\n",
    "mean_train = np.array(mean_train).flatten()\n",
    "std_train = np.array(std_train).flatten()\n",
    "actual_train = np.array(actual_train).flatten()\n",
    "mean_test = np.array(mean_test).flatten()\n",
    "std_test = np.array(std_test).flatten()\n",
    "actual_test = np.array(actual_test).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "confidence_levels = np.arange(0, 1.05, 0.05)\n",
    "train_metrics = calculate_metrics(actual_train, mean_train, std_train, confidence_levels)\n",
    "test_metrics = calculate_metrics(actual_test, mean_test, std_test, confidence_levels)\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Dataset': ['Training', 'Test'],\n",
    "    'MAE': [train_metrics['MAE'], test_metrics['MAE']],\n",
    "    'RMSE': [train_metrics['RMSE'], test_metrics['RMSE']],\n",
    "    'R2': [train_metrics['R2'], test_metrics['R2']],\n",
    "    'Spearman': [train_metrics['Spearman'], test_metrics['Spearman']],\n",
    "    'Calibration Area': [train_metrics['Calibration Area'], test_metrics['Calibration Area']]\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "create_errorbar_plot(actual_train, mean_train, std_train, 'blue', 'Training')\n",
    "create_errorbar_plot(actual_test, mean_test, std_test, 'green', 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot additional figures\n",
    "abs_error_train = np.abs(actual_train - mean_train)\n",
    "abs_error_test = np.abs(actual_test - mean_test)\n",
    "\n",
    "plot_abs_error_vs_std(abs_error_train, std_train, 'Training', 'blue')\n",
    "plot_abs_error_vs_std(abs_error_test, std_test, 'Test', 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_std_histogram(std_train, 'Training', 'blue')\n",
    "plot_std_histogram(std_test, 'Test', 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate observed confidence\n",
    "observed_confidence_train = calculate_observed_confidence(actual_train, mean_train, std_train, confidence_levels)\n",
    "observed_confidence_test = calculate_observed_confidence(actual_test, mean_test, std_test, confidence_levels)\n",
    "\n",
    "# Plot calibration curves\n",
    "plot_calibration_curve(confidence_levels, observed_confidence_train, 'Training')\n",
    "plot_calibration_curve(confidence_levels, observed_confidence_test, 'Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "active_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
