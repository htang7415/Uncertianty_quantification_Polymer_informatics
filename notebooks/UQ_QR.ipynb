{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../functions')\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Import custom modules\n",
    "from data_processing import load_and_process_data\n",
    "from models import QR_Model, quantile_loss, predictions_QR\n",
    "from metrics import calculate_metrics, calculate_observed_confidence\n",
    "from post_processing import create_errorbar_plot, plot_abs_error_vs_std, plot_std_histogram, plot_calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 800\n",
    "HIDDEN_LAYERS = [1024, 256, 128]\n",
    "LEARNING_RATE = 0.000015\n",
    "QUANTILES = [0.1, 0.5, 0.9]  # We'll use 0.5 as median (our prediction) and 0.1, 0.9 to create uncertainty bounds\n",
    "num_zero_threshold = 3600\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load and process data\n",
    "file_path = \"../data/other_property/Tm.csv\"\n",
    "X_count, Y = load_and_process_data(file_path, num_zero_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "xtrain, xtemp, ytrain, ytemp = train_test_split(X_count, Y, test_size=0.2, random_state=11)\n",
    "xval, xtest, yval, ytest = train_test_split(xtemp, ytemp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_dataloader(x, y, batch_size):\n",
    "    tensor_x = torch.FloatTensor(x.values).to(device)\n",
    "    tensor_y = torch.FloatTensor(y).to(device)\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "    return torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader = create_dataloader(xtrain, ytrain, BATCH_SIZE)\n",
    "val_loader = create_dataloader(xval, yval, BATCH_SIZE)\n",
    "test_loader = create_dataloader(xtest, ytest, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_shape = xtrain.shape[1]\n",
    "model = QR_Model(input_shape, QUANTILES).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(batch_x)\n",
    "        loss = quantile_loss(predictions, batch_y, QUANTILES)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'Training completed in {total_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "mean_train, std_train = predictions_QR(model, train_loader)\n",
    "mean_test, std_test = predictions_QR(model, test_loader)\n",
    "\n",
    "# Calculate metrics\n",
    "confidence_levels = np.arange(0, 1.05, 0.05)\n",
    "train_metrics = calculate_metrics(ytrain, mean_train, std_train, confidence_levels)\n",
    "test_metrics = calculate_metrics(ytest, mean_test, std_test, confidence_levels)\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Dataset': ['Training', 'Test'],\n",
    "    'MAE': [train_metrics['MAE'], test_metrics['MAE']],\n",
    "    'RMSE': [train_metrics['RMSE'], test_metrics['RMSE']],\n",
    "    'R2': [train_metrics['R2'], test_metrics['R2']],\n",
    "    'Spearman': [train_metrics['Spearman'], test_metrics['Spearman']],\n",
    "    'Calibration Area': [train_metrics['Calibration Area'], test_metrics['Calibration Area']]\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "create_errorbar_plot(ytrain, mean_train, std_train, 'blue', 'Training')\n",
    "create_errorbar_plot(ytest, mean_test, std_test, 'green', 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot additional figures\n",
    "abs_error_train = np.abs(ytrain - mean_train)\n",
    "abs_error_test = np.abs(ytest - mean_test)\n",
    "\n",
    "plot_abs_error_vs_std(abs_error_train, std_train, 'Training', 'blue')\n",
    "plot_abs_error_vs_std(abs_error_test, std_test, 'Test', 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_std_histogram(std_train, 'Training', 'blue')\n",
    "plot_std_histogram(std_test, 'Test', 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate observed confidence\n",
    "observed_confidence_train = calculate_observed_confidence(ytrain, mean_train, std_train, confidence_levels)\n",
    "observed_confidence_test = calculate_observed_confidence(ytest, mean_test, std_test, confidence_levels)\n",
    "\n",
    "# Plot calibration curves\n",
    "plot_calibration_curve(confidence_levels, observed_confidence_train, 'Training')\n",
    "plot_calibration_curve(confidence_levels, observed_confidence_test, 'Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
